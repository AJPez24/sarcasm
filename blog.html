<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>My CS Project Blog</title>
    <style>
        body {
            margin: 0;
            font-family: system-ui, sans-serif;
            background: #f7f7f9;
            color: #222;
            padding: 2rem;
            display: flex;
            justify-content: center;
        }

        .wrapper {
            max-width: 700px;
            background: white;
            padding: 2rem;
            border-radius: 14px;
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.06);
        }

        h1 {
            margin-top: 0;
            font-size: 2rem;
            color: rgb(59, 59, 166)
        }

        h2 {
            color: rgb(24, 24, 120)
        }

        .meta {
            color: #777;
            font-size: .9rem;
            margin-bottom: 1.5rem;
        }

        p {
            line-height: 1.6;
            margin: 1rem 0;
        }

        footer {
            margin-top: 2rem;
            padding-top: 1rem;
            border-top: 1px solid #eee;
            color: #777;
            font-size: .85rem;
            text-align: center;
        }
    </style>
</head>

<body>
    <div class="wrapper">
        <h1>Sarcasm Detector</h1>
        <div class="meta">Allie Vorley, AJ Pezza, Jackie Soloveychik, Tara Udani</div>

        <hr>

        <h2>Background & Motivation</h2>
        <p>Machine learning models notoriously struggle to understand and react appropriately to sarcastic statements. 
            Sarcasm often conveys a sentiment opposite to the actual meaning, and models mimic human language 
            without sentience. As a result, words that change meanings across different contexts, like “not,” are extremely 
            difficult to interpret. </p>
            
        <p>Our goal is to increase AI’s semantic comprehension by creating a sarcasm detector 
            that can be used as a filter for sentiment analysis models, specifically for short content. Our project 
            explores the question: Can an artificial network effectively detect sarcasm in short text content based 
            on linguistic and contextual clues?</p>

        <h2>Data & Resources</h2>
        <p>Our main data source is the Self-Annotated Reddit Corpus (SARC), organized into JSON and CSV files which are 
            publicly available 
            on Github. The JSON file includes 1.3 million sarcastic comments labeled by the authors, 
            with text and metadata for each comment.  In addition to the text and id, the 
            metadata includes seven features: author, score, ups, downs, date, created utc, 
            subreddit. A separate CSV file contains train and test data showing context and sarcasm labels. 
            It includes a sequence of comments following a post and a set of responses to the last comment 
            in that sequence (all labeled by id number).</p>
        <p>We preprocessed the data to create a new flattened data frame combining the text 
            (contained in the JSON file) and sarcasm labels (contained in the CSV file). We 
            started by reducing the JSON to contain only comments that matched IDs present in the CSV. We 
            then created a new CSV file with only the id, text, and sarcasm label.</p>
        <p>We are using the Hugging Face transformers model BERT (Bidirection Encoder Representations 
            from Transformers) as a feature extractor. BERT is a pre-trained large language model 
            trained that uses masked language modeling (masking 15% of words in a sentence and trying 
            to predict them) and next sentence prediction (concatenating two random sentences and 
            predicting if they were next to each other in the original text). We used BERT to create 
            embeddings to represent the labeled text data.</p>

        <h2>Design</h2>
        <p>After preprocessing data, we used BERT as a feature extractor to represent the raw text 
            as token embeddings. We provided the text as input, and receive a single 768-dimensional 
            vector as output to represent the full text. This results in a dataset with shape N x 768,
             where each row is a fixed-length embedding.</p>
        <p>The embeddings serve as input in our classification model, which uses a fully-connected 
            feed-forward neural network. Our initial model structure uses three dense layers; the 
            first two reduce the level of dimensions, compressing the features as the model learns, 
            and the third produces a binary output. Two dropout layers between reduce overfitting.</p>
        <img src="images/model-info.png" style="width: 100%">
        <p>(talk about revisions to improve accuracy and reduce overfitting, analysis techniques)</p>

        <h2>Results</h2>
        <p>Outline the results and findings of the project, plots and figures to show the results - 
            ensure they are clear, concise, and relevant.</p>

        <h2>Discussion</h2>
        <p>Discuss the context of your results, how the results relate to each other and how to interpret them.
            Revisit challenges you encountered and how you mitigated or solved them.
            If applicable, put your project in relation to other work in the same field.
            </p>
            
        <h2>Conclusion</h2>
        <p>Summarize the main results and implications of your project.</p>

        <footer>
            © 2025 | Artificial Neural Networks
        </footer>
    </div>
</body>

</html>