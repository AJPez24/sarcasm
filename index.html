<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Sarcasm Detector</title>
    <style>
        body {
            margin: 0;
            font-family: system-ui, sans-serif;
            background: #dee3ed;
            color: #222;
            padding: 2rem;
            display: flex;
            justify-content: center;
        }

        .wrapper {
            max-width: 700px;
            background: white;
            padding: 2rem;
            border-radius: 14px;
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.06);
        }

        h1 {
            margin-top: 0;
            font-size: 2rem;
            color: #465baf
        }

        h2 {
            color: #1e3e7e
        }

        .meta {
            color: #777;
            font-size: .9rem;
            margin-bottom: 1.5rem;
        }

        p {
            line-height: 1.6;
            margin: 1rem 0;
        }

        footer {
            margin-top: 2rem;
            padding-top: 1rem;
            border-top: 1px solid #eee;
            color: #777;
            font-size: .85rem;
            text-align: center;
        }
    </style>
</head>

<body>
    <div class="wrapper">
        <h1>Sarcasm Detector</h1>
        <div class="meta">Allie Vorley, AJ Pezza, Jackie Soloveychik, Tara Udani</div>

        <hr>

        <h2>Background & Motivation</h2>
        <p>Machine learning models notoriously struggle to understand and react appropriately to sarcastic statements. 
            Sarcasm often conveys a sentiment opposite to the actual meaning, and models mimic human language 
            without sentience. As a result, words that change meanings across different contexts, like “not,” are extremely 
            difficult to interpret. </p>
            
        <p>Our goal is to increase AI’s semantic comprehension by creating a sarcasm detector 
            that can be used as a filter for sentiment analysis models, specifically for short content. Our project 
            explores the question: Can an artificial network effectively detect sarcasm in short text content based 
            on linguistic and contextual clues?</p>

        <h2>Data & Resources</h2>
        <p>Our main data source is the Self-Annotated Reddit Corpus (SARC), organized into JSON and CSV files which are 
            publicly available 
            on Github. The JSON file includes 1.3 million sarcastic comments labeled by the authors, 
            with text and metadata for each comment.  In addition to the text and id, the 
            metadata includes seven features: author, score, ups, downs, date, created utc, 
            subreddit. A separate CSV file contains train and test data showing context and sarcasm labels. 
            It includes a sequence of comments following a post and a set of responses to the last comment 
            in that sequence (all labeled by id number).</p>
        <p>We preprocessed the data to create a new flattened data frame combining the text 
            (contained in the JSON file) and sarcasm labels (contained in the CSV file). We 
            started by reducing the JSON to contain only comments that matched IDs present in the CSV. We 
            then created a new CSV file with only the id, text, and sarcasm label.</p>
        <p>(add visualizations of the data - distribution, group by author & subreddit)</p>
        <p>We are using the Hugging Face transformers model BERT (Bidirection Encoder Representations 
            from Transformers) as a feature extractor. BERT is a pre-trained large language model 
            that uses masked language modeling (masking 15% of words in a sentence and trying 
            to predict them) and next sentence prediction (concatenating two random sentences and 
            predicting if they were next to each other in the original text). We used BERT to create 
            embeddings to represent the labeled text data.</p>

        <h2>Initial Design</h2>
        <p>After preprocessing data, we used BERT as a feature extractor to represent the raw text 
            as token embeddings. We provided the text as input, and received a single 768-dimensional 
            vector as output to represent the full text. This results in a dataset with the shape N x 768,
             where each row is a fixed-length embedding.</p>
        <p>The embeddings serve as input in our classification model, which uses a fully-connected 
            feed-forward neural network. Our initial model structure uses three dense layers; the 
            first two reduce the level of dimensions, compressing the features as the model learns, 
            and the third produces a binary output. Two dropout layers between reduce overfitting.</p>
        <img src="images/model-info.png" style="width: 100%">

        <p>analysis techniques: one hot model - tokenize text to create 5000-dimension vector for text 
            - one-hot bag-of-words (each word assigned 0 or 1 based on whether it is present or not, 
            w/ no info about context or word order),
            feed into basic neural network, creates a baseline to compare our model to</p>

        <h2>Revisions</h2>

        <p>In order to improve the accuracy of our model and reduce overfitting, we applied several changes
            to our initial model.
        </p>
        <p>(add images of initial model results)</p>
        <p>We switched to mean pooling to get more accurate embeddings from BERT. Previously, we solely used
            a classification token. Mean pooling allows us to get a vector for every token in the sentence, 
            then take the average to produce a vector representing the sentence. This method creates 
            better semantic similarity to fully represent the entire text.</p>
        <p>In the model, we added another dense layer to create a more gradual compression, allowing the model to 
            learn more features before classifying the text embedding. We also switched from a constant dropout 
            to decreasing rates of dropout on the first three dense layers. We used a lower dropout rate toward the end 
            because later layers are smaller and encode more specific information.</p>
        <p>We switched the optimizer from adam to adamW (explain more about what this does)</p>
        <p>We added two callbacks using Keras. The first, reducing learning rate on plateau, monitors validation loss, 
            and decreases the learning rate if val loss does not improve for 2 epochs. The second, early stopping, 
            also monitors val loss. If it does not improve for 4 epochs, the model stops training and uses weights 
            from the epoch with the lowest validation loss.</p>
        <p>Finally, we reduced the batch size and increased the number of epochs. The smaller batch size helps with
             regularization and reduces overfitting, and a higher number of epochs balances out the smaller batch 
             size. The inreased epochs allows the model to use them if needed, but it also uses early stopping when 
            val loss stops improving.</p>
        <img src="images/model-revised-info.png" style="width: 100%">

        <h2>Results</h2>
        <p>Outline the results and findings of the project, plots and figures to show the results - 
            ensure they are clear, concise, and relevant.</p>

        <h2>Discussion</h2>
        <p>Discuss the context of your results, how the results relate to each other and how to interpret them.
            Revisit challenges you encountered and how you mitigated or solved them.
            If applicable, put your project in relation to other work in the same field.
            </p>
            
        <h2>Conclusion</h2>
        <p>Summarize the main results and implications of your project.</p>

        <footer>
            © 2025 | Artificial Neural Networks
        </footer>
    </div>
</body>

</html>