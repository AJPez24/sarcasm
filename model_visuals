#INITIAL MODEL

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

data = np.load("./data/bert_embeddings.npz")

X = data["embeddings"]      # shape (N, 768)
y = data["labels"]          # shape (N,)

print("Embeddings:", X.shape)
print("Labels:", y.shape)

# train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42, stratify=y
)
# model structure
model = Sequential([
    Dense(512, activation="relu", input_shape=(768,)),
    Dropout(0.2),
    Dense(128, activation="relu"),
    Dropout(0.2),
    Dense(1, activation="sigmoid")  # binary output
])

# try adding more layers ... larger neuron filtering for both layers?
# change dropout to 0.5 maybe 

# mocel compilation
model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

model.summary()

# fitting the model 
history = model.fit(
    X_train,
    y_train,
    batch_size=32,
    epochs=30,
    validation_split=0.1,
    verbose=1
)

import seaborn as sns

#setting theme for plots
sns.set_theme(style = "ticks")

#confusion matrix
y_pred = (model.predict(X_test) > 0.5).astype(int)
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Reds")
plt.title("Initial Model Confusion Matrix - 30 Epochs ")
plt.show()

#probability histogram
y_prob = model.predict(X_test).ravel()
plt.hist(y_prob, bins=30, color='darkred')
plt.title("Initial Model Predicted Probability Distribution")
plt.xlabel("Predicted Probability")
plt.ylabel("Frequency")
sns.despine()
plt.show()


#loss plot
plt.plot(history.history["loss"], label="Train Loss", color='deepskyblue')
plt.plot(history.history["val_loss"], label="Val Loss", color = 'darkred')
plt.title("Initial Model Loss")
plt.xlabel("Number of Epochs")
sns.despine()
plt.ylabel("Loss")
plt.legend()
plt.show()

#FINAL MODEL

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler

# load embeddings
train_data = np.load("./data/train_embeddings_mean.npz")
test_data = np.load("./data/test_embeddings_mean.npz")

x_train = train_data["embeddings"]      # shape (N, 768)
y_train = train_data["labels"]          # shape (N,)

x_test = test_data["embeddings"]      # shape (N, 768)
y_test = test_data["labels"]          # shape (N,)

print("Embeddings:", x_train.shape)
print("Labels:", y_train.shape)


model = Sequential([
    Dense(512, activation="relu", input_shape=(768,)),
    Dropout(0.3),

    Dense(256, activation="relu"),
    Dropout(0.3),

    Dense(64, activation="relu"),
    Dropout(0.3),

    Dense(1, activation="sigmoid")
])

smoothing_loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)

adamw = tf.keras.optimizers.AdamW(
            learning_rate=3e-4,
            weight_decay=3e-4 
        )

model.compile(
    loss=smoothing_loss,
    optimizer=adamw,
    metrics=["accuracy"]
)


callbacks = [
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=2,
        min_lr=1e-6,
        verbose=1
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=10,
        restore_best_weights=True,
        verbose=1
    )
]

model.summary()

# fitting the model 
history = model.fit(
    x_train,
    y_train,
    batch_size=16,
    epochs=20,          # let callbacks stop early
    validation_split=0.1,
    callbacks=callbacks,
    verbose=1
)


sns.set_theme(style = "ticks")
#confusion matrix
#y_pred = (model.predict(X_test) > 0.5).astype(int)
#cm = confusion_matrix(y_test, y_pred)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap="Greens")
#plt.title("Final Model Confusion Matrix")
#plt.show()

#probability histogram
y_prob = model.predict(X_test).ravel()
plt.hist(y_prob, bins=10, color='darkgreen')
plt.title("Final Model Predicted Probability Distribution")
plt.xlabel("Predicted Probability")
plt.ylabel("Frequency")
sns.despine()
plt.show()


#loss plot
plt.plot(history.history["loss"], label="Train Loss", color='darkorange')
plt.plot(history.history["val_loss"], label="Val Loss", color = 'darkgreen')
plt.title("Final Model Loss")
plt.xlabel("Number of Epochs")
sns.despine()
plt.ylabel("Loss")
plt.legend()
plt.show()
